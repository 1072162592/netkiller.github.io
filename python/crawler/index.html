<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>第 11 章 Crawler</title><link rel="stylesheet" type="text/css" href="..//docbook.css" /><meta name="generator" content="DocBook XSL Stylesheets V1.79.1" /><meta name="keywords" content="php, pear, pecl, phar" /><link rel="home" href="../index.html" title="Netkiller Python 手札" /><link rel="up" href="../index.html" title="Netkiller Python 手札" /><link rel="prev" href="../library/zope.testbrowser.html" title="10.18. zope.testbrowser" /><link rel="next" href="../3rdparty/index.html" title="第 12 章 3rdparty toolkit" /></head><body><a xmlns="" href="//www.netkiller.cn/">Home</a> |
		<a xmlns="" href="//netkiller.github.io/">简体中文</a> |
	    <a xmlns="" href="http://netkiller.sourceforge.net/">繁体中文</a> |
	    <a xmlns="" href="/journal/index.html">杂文</a> |
	    <a xmlns="" href="//www.netkiller.cn/home/donations.html">打赏(Donations)</a> |
	    <a xmlns="" href="http://netkiller-github-com.iteye.com/">ITEYE 博客</a> |
	    <a xmlns="" href="http://my.oschina.net/neochen/">OSChina 博客</a> |
	    <a xmlns="" href="https://www.facebook.com/bg7nyt">Facebook</a> |
	    <a xmlns="" href="http://cn.linkedin.com/in/netkiller/">Linkedin</a> |
	    <a xmlns="" href="https://zhuanlan.zhihu.com/netkiller">知乎专栏</a> |
	    <a xmlns="" href="/search.html">Search</a> |
		<a xmlns="" href="mailto:netkiller@msn.com">Email</a><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">第 11 章 Crawler</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="../library/zope.testbrowser.html">上一页</a> </td><th width="60%" align="center"> </th><td width="20%" align="right"> <a accesskey="n" href="../3rdparty/index.html">下一页</a></td></tr></table><hr /></div><table xmlns=""><tr><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=watch&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;repo=netkiller.github.io&amp;type=fork&amp;count=true&amp;size=large" height="30" width="170" frameborder="0" scrolling="0" style="width:170px; height: 30px;" allowTransparency="true"></iframe></td><td><iframe src="//ghbtns.com/github-btn.html?user=netkiller&amp;type=follow&amp;count=true&amp;size=large" height="30" width="240" frameborder="0" scrolling="0" style="width:240px; height: 30px;" allowTransparency="true"></iframe></td></tr></table><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a id="index"></a>第 11 章 Crawler</h1></div></div></div><div class="toc"><p><strong>目录</strong></p><dl class="toc"><dt><span class="section"><a href="index.html#scrapy">11.1. scrapy</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#setup">11.1.1. 安装 scrapy 开发环境</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#idp48">11.1.1.1. Mac</a></span></dt><dt><span class="section"><a href="index.html#idp49">11.1.1.2. Ubuntu</a></span></dt><dt><span class="section"><a href="index.html#idp50">11.1.1.3. 使用 pip 安装 scrapy</a></span></dt><dt><span class="section"><a href="index.html#idp51">11.1.1.4. 测试 scrapy</a></span></dt></dl></dd><dt><span class="section"><a href="index.html#shell">11.1.2. 爬虫 Shell</a></span></dt><dt><span class="section"><a href="index.html#idp53">11.1.3. 爬虫项目</a></span></dt><dd><dl><dt><span class="section"><a href="index.html#idp52">11.1.3.1. 创建项目</a></span></dt></dl></dd></dl></dd></dl></div>
	
<div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="scrapy"></a>11.1. scrapy</h2></div></div></div>
	
	<p>https://scrapy.org</p>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="setup"></a>11.1.1. 安装 scrapy 开发环境</h3></div></div></div>
		
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="idp48"></a>11.1.1.1. Mac</h4></div></div></div>
			
			<pre class="screen">
neo@MacBook-Pro ~ % brew install python3
neo@MacBook-Pro ~ % pip3 install scrapy
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="idp49"></a>11.1.1.2. Ubuntu</h4></div></div></div>
			
			<p>搜索 scrapy 包，scrapy 支持 Python2.7 和 Python3 我们只需要 python3 版本</p>
			<pre class="screen">
neo@netkiller ~ % apt-cache search scrapy | grep python3
python3-scrapy - Python web scraping and crawling framework (Python 3)
python3-scrapy-djangoitem - Scrapy extension to write scraped items using Django models (Python3 version)
python3-w3lib - Collection of web-related functions (Python 3)			
			</pre>
			<p>Ubuntu 17.04 默认 scrapy 版本为 1.3.0-1 如果需要最新的 1.4.0 请使用 pip 命令安装</p>
			<pre class="screen">
neo@netkiller ~ % apt search python3-scrapy
Sorting... Done
Full Text Search... Done
python3-scrapy/zesty,zesty 1.3.0-1~exp2 all
  Python web scraping and crawling framework (Python 3)

python3-scrapy-djangoitem/zesty,zesty 1.1.1-1 all
  Scrapy extension to write scraped items using Django models (Python3 version)
			</pre>
			<p>安装 scrapy</p>
			<pre class="screen">
neo@netkiller ~ % sudo apt install python3-scrapy
[sudo] password for neo: 
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  ipython3 libmysqlclient20 libwebpmux2 mysql-common python-pexpect python-ptyprocess python3-attr python3-boto python3-bs4 python3-cffi-backend python3-click python3-colorama python3-constantly
  python3-cryptography python3-cssselect python3-decorator python3-html5lib python3-idna python3-incremental python3-ipython python3-ipython-genutils python3-libxml2 python3-lxml python3-mysqldb
  python3-openssl python3-pam python3-parsel python3-pexpect python3-pickleshare python3-pil python3-prompt-toolkit python3-ptyprocess python3-pyasn1 python3-pyasn1-modules python3-pydispatch
  python3-pygments python3-queuelib python3-serial python3-service-identity python3-setuptools python3-simplegeneric python3-traitlets python3-twisted python3-twisted-bin python3-w3lib python3-wcwidth
  python3-webencodings python3-zope.interface
Suggested packages:
  python-pexpect-doc python-attr-doc python-cryptography-doc python3-cryptography-vectors python3-genshi python3-lxml-dbg python-lxml-doc default-mysql-server | virtual-mysql-server
  python-egenix-mxdatetime python3-mysqldb-dbg python-openssl-doc python3-openssl-dbg python3-pam-dbg python-pil-doc python3-pil-dbg doc-base python-pydispatch-doc ttf-bitstream-vera python-scrapy-doc
  python3-wxgtk3.0 | python3-wxgtk python-setuptools-doc python3-tk python3-gtk2 python3-glade2 python3-qt4 python3-wxgtk2.8 python3-twisted-bin-dbg
The following NEW packages will be installed:
  ipython3 libmysqlclient20 libwebpmux2 mysql-common python-pexpect python-ptyprocess python3-attr python3-boto python3-bs4 python3-cffi-backend python3-click python3-colorama python3-constantly
  python3-cryptography python3-cssselect python3-decorator python3-html5lib python3-idna python3-incremental python3-ipython python3-ipython-genutils python3-libxml2 python3-lxml python3-mysqldb
  python3-openssl python3-pam python3-parsel python3-pexpect python3-pickleshare python3-pil python3-prompt-toolkit python3-ptyprocess python3-pyasn1 python3-pyasn1-modules python3-pydispatch
  python3-pygments python3-queuelib python3-scrapy python3-serial python3-service-identity python3-setuptools python3-simplegeneric python3-traitlets python3-twisted python3-twisted-bin python3-w3lib
  python3-wcwidth python3-webencodings python3-zope.interface
0 upgraded, 49 newly installed, 0 to remove and 0 not upgraded.
Need to get 7,152 kB of archives.
After this operation, 40.8 MB of additional disk space will be used.
Do you want to continue? [Y/n]
			</pre>
			<p>输入大写 “Y” 然后回车</p>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="idp50"></a>11.1.1.3. 使用 pip 安装 scrapy</h4></div></div></div>
			
			<pre class="screen">
neo@netkiller ~ % sudo apt install python3-pip
neo@netkiller ~ % pip3 install scrapy
			</pre>
		</div>
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="idp51"></a>11.1.1.4. 测试 scrapy</h4></div></div></div>
			
			<p>创建测试程序，用于验证 scrapy 安装是否存在问题。</p>
			<pre class="screen">
			
$ cat &gt; myspider.py &lt;&lt;EOF
import scrapy

class BlogSpider(scrapy.Spider):
    name = 'blogspider'
    start_urls = ['https://blog.scrapinghub.com']

    def parse(self, response):
        for title in response.css('h2.entry-title'):
            yield {'title': title.css('a ::text').extract_first()}

        for next_page in response.css('div.prev-post &gt; a'):
            yield response.follow(next_page, self.parse)
EOF
			
			</pre>
			<p>运行爬虫</p>
			<pre class="screen">
$ scrapy runspider myspider.py
			</pre>
		</div>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="shell"></a>11.1.2. 爬虫 Shell</h3></div></div></div>
		
		<pre class="screen">
		
neo@MacBook-Pro /tmp % scrapy shell http://www.netkiller.cn
2017-09-01 15:23:05 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)
2017-09-01 15:23:05 [scrapy.utils.log] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}
2017-09-01 15:23:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2017-09-01 15:23:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-09-01 15:23:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-09-01 15:23:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-09-01 15:23:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-09-01 15:23:05 [scrapy.core.engine] INFO: Spider opened
2017-09-01 15:23:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.netkiller.cn&gt; (referer: None)
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x103b2afd0&gt;
[s]   item       {}
[s]   request    &lt;GET http://www.netkiller.cn&gt;
[s]   response   &lt;200 http://www.netkiller.cn&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x1049019e8&gt;
[s]   spider     &lt;DefaultSpider 'default' at 0x104be2a90&gt;
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects 
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
&gt;&gt;&gt; 
		
		</pre>
	</div>
	<div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="idp53"></a>11.1.3. 爬虫项目</h3></div></div></div>
		
		<div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="idp52"></a>11.1.3.1. 创建项目</h4></div></div></div>
			
			<p>创建爬虫项目</p>
			<pre class="screen">
scrapy startproject project
			</pre>
			<pre class="screen">
			
neo@MacBook-Pro /tmp % scrapy startproject project
New Scrapy project 'project', using template directory '/usr/local/lib/python3.6/site-packages/scrapy/templates/project', created in:
    /private/tmp/project

You can start your first spider with:
    cd project
    scrapy genspider example example.com
neo@MacBook-Pro /tmp % cd project/ 
neo@MacBook-Pro /tmp/project % find .
.
./project
./project/__init__.py
./project/__pycache__
./project/items.py
./project/middlewares.py
./project/pipelines.py
./project/settings.py
./project/spiders
./project/spiders/__init__.py
./project/spiders/__pycache__
./scrapy.cfg
			
			
			</pre>
		</div>
	</div>
</div>
</div><div xmlns="" id="disqus_thread"></div><script xmlns="">

var disqus_config = function () {
this.page.url = "http://www.netkiller.cn";  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = 'netkiller'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//netkiller.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script><noscript xmlns="">Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><br xmlns="" /><script xmlns="" type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=r5HG&amp;d=9mi5r_kkDC8uxG8HuY3p4-2qgeeVypAK9vMD-2P6BYM"></script><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="../library/zope.testbrowser.html">上一页</a> </td><td width="20%" align="center"> </td><td width="40%" align="right"> <a accesskey="n" href="../3rdparty/index.html">下一页</a></td></tr><tr><td width="40%" align="left" valign="top">10.18. zope.testbrowser </td><td width="20%" align="center"><a accesskey="h" href="../index.html">起始页</a></td><td width="40%" align="right" valign="top"> 第 12 章 3rdparty toolkit</td></tr></table></div><script xmlns="">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-11694057-1', 'auto');
  ga('send', 'pageview');

</script><script xmlns="" async="async">
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?93967759a51cda79e49bf4e34d0b0f2c";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script xmlns="" async="async">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script><script xmlns="" type="text/javascript" src="/js/q.js" async="async"></script></body></html>